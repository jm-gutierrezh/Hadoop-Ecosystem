{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop\n",
    "\n",
    "Hadoop is a distributed computing technology, which allows jobs to be processed in server farms, groups of connected computers.\n",
    "\n",
    "There is a single co-ordinating software which must perform the following tasks:\n",
    "\n",
    "* Partition data\n",
    "* Co-ordinate computing tasks\n",
    "* Handle fault tolerance and recovery\n",
    "* Allocate capacity to different processes\n",
    "\n",
    "Hadoop is composed of three base technologies:\n",
    "\n",
    "* `HDFS` (Hadoop Distributed File System), a file system to manage **distributed storage**.\n",
    "* `MapReduce`, a framework to manage **distributed computing**.\n",
    "* `YARN` (Yet Another Resource Negotiator), a framework to run and manage the data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS\n",
    "\n",
    "Hadoop Distributed File System\n",
    "\n",
    "* Highly fault tolerant\n",
    "* Suited to batch processing - data access has high throughput rather than low latency\n",
    "* Supports very large data sets\n",
    "\n",
    "How does HDFS manage file distribution?\n",
    "\n",
    "One of the nodes in the cluster is selected as the master node, or **Name Node**, which contains all the metadata required to co-ordinate the other nodes. The Name node stores the directory structure and the metadata of the files, not the files themselves.\n",
    "\n",
    "HDFS stores files in blocks of 128 MB, which are then stored in a distributed way in the cluster. Block size offers a tradeoff between parallelism and overhead. A larger block size decreases parallelism but increases overhead.\n",
    "\n",
    "To read files in HDFS:\n",
    "\n",
    "1. Use metadata in the Name node to lookup block locations.\n",
    "2. Read the blocks from their respective locations.\n",
    "\n",
    "To allow for fault tolerance (file corruption or node crashing):\n",
    "\n",
    "1. Replicate blocks based on the replication factor (usually 3).\n",
    "2. Store replicas in different locations.\n",
    "\n",
    "Replication balances a tradeoff between redundancy and write bandwidth. More replication increases redundancy (and fault tolerance), but decreases write bandwidth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "MapReduce is a programming paradigm for distributed computing. Consists of two operations: Map and Reduce.\n",
    "\n",
    "Map is a parallel operation which takes in a record and returns a (key, value) pair.\n",
    "\n",
    "Combines the results of the Map operations by their keys and returns a result.\n",
    "\n",
    "Usually, MapReduce jobs are programmed in `Java`, by implementing three classes:\n",
    "\n",
    "* Mapper class\n",
    "* Reducer class\n",
    "* Main class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YARN\n",
    "\n",
    "Yet Another Resource Negotiator, is a program in charge of co-ordinating tasks running on the cluster.\n",
    "\n",
    "YARN has two distinct subcomponents: \n",
    "\n",
    "|Resource Manager|Node Manager|\n",
    "|---|---|\n",
    "|Runs on a single master node|Runs on all the other nodes|\n",
    "|Schedules tasks across nodes|Manages tasks on the individual node|\n",
    "\n",
    "The job submission process is as follows:\n",
    "\n",
    "1. A MapReduce job is submitted to the ResourceManager\n",
    "2. The Resource Manager finds a NodeManager with free capacity\n",
    "3. The NodeManager runs the job\n",
    "4. NodeManagers can request containers for Mappers and Reducers, or request CPU or memory requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive\n",
    "\n",
    "Hive provides an SQL interface to Hadoop which runs all of its internal processes as MapReduce jobs.\n",
    "\n",
    "Hive exposes files in HDFS in the form of tables to the user through a component known as the Hive metastore, which is basically a bridge between HDFS and Hive.\n",
    "\n",
    "The Hive metastore:\n",
    "\n",
    "* Usually is a relational database itself\n",
    "* Stores metadata for a tables in Hive\n",
    "* Maps the files and directories in Hive to tables\n",
    "* Holds table definitions and the schema for each table\n",
    "* Any database with a JDBC driver can be used as a metastore\n",
    "\n",
    "#### Hive vs RDBMS\n",
    "\n",
    "|Factor|Hive|RDBMS|\n",
    "|---|---|---|\n",
    "|Data size|Large datasets (petabytes)|Small datasets (gigabytes)|\n",
    "|Computation|Parallel computation (horizontal scaling)|Serial computation (vertical scaling)|\n",
    "|Latency|High latency (no indexing)|Low latency (indexing)|\n",
    "|Operations|Force schema-on-read|Force schema-on-write|\n",
    "|ACID compliance|Yes (data can be dumped into tables from any source)|No (strict check for storing)|\n",
    "|Query language|HiveQL|SQL|\n",
    "\n",
    "#### Partitioning\n",
    "\n",
    "Allows for data to be split into logical units, each unit will be stored in a different directory. Queries referencing partitions will run only on the required directories.\n",
    "\n",
    "Note that partitioned units are not necessarily the same size.\n",
    "\n",
    "#### Bucketing\n",
    "\n",
    "Bucketing allows for partition of data into equally-sized units, each unit will be stored in a different file. Bucketing accomplished equal-size partitioning through hashing a column value.\n",
    "\n",
    "#### Queries on Big Data\n",
    "\n",
    "* Partitioning and bucketing of tables\n",
    "* Join optimizations through reduction of required memory\n",
    "  * In joins, one table is held in memory while the other is read from disk\n",
    "  * For best performance, hold the smaller table in memory\n",
    "  * Another optimization is to write joins as map-only operations\n",
    "* Window functions make complex operations simple without needing many intermediate calculations, through the definition of a **window** and an **operation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HBase\n",
    "\n",
    "A database management system on top of Hadoop. Integrates with applications just like a traditional database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig\n",
    "\n",
    "Pig is a procedural, data flow language to transform unstructured or inconsistent data into a structured format. Is used to get data into the data warehouse.\n",
    "\n",
    "Is focused on sequential transformations applied to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "A distributed computing engine used to quickly process datasets. Has a bunch of built-in libraries for machine learning, stream processing and graph processing.\n",
    "\n",
    "It is a general purpose engine for exploring, cleaning and preparing data, applying machine learning and building data applications.\n",
    "\n",
    "Data in Spark is abstracted through RDDs or **Resilient Distributed Datasets**, which are in-memory collections of objects.\n",
    "\n",
    "The basic functionality of Spark is the **Spark Core**, which is the computing engine. The Spark Core requires two additional components: the **Storage System** (HDFS), which stores the data to be processed, and the **Cluster Manager** (YARN), which helps Spark run tasks across a cluster of machines. In this way, Spark replaces MapReduce.\n",
    "\n",
    "The **SparkContext** represents a connection to the Spark Cluster (which is the Hadoop cluster, as now we're using the Storage System and Cluster Manager).\n",
    "\n",
    "#### RDDs\n",
    "\n",
    "* The core abstraction of Spark\n",
    "* RDDs don't execute, they just store metadata\n",
    "* Partitioning\n",
    "  * Data is divided into partitions\n",
    "  * Distributed to multiple machines\n",
    "* Immutability\n",
    "  * RDDs are read-only\n",
    "  * Only two types of operations: Transformations and Actions\n",
    "  * Transformations transform the RDD into another RDD\n",
    "  * Actions request results and materialize the RDDs\n",
    "* Lineage\n",
    "  * Every RDD remember its previous transformations\n",
    "  \n",
    "#### Lazy Evaluation\n",
    "\n",
    "1. Spark keeps a record of the series of transformations requested, but doesn't evaluate them\n",
    "2. Spark groups the transformations in an efficient way when an Action is requested\n",
    "\n",
    "#### Installing Spark\n",
    "\n",
    "1. Download Spark binaries\n",
    "2. Update environment variables\n",
    "   * SPARK_HOME\n",
    "   * PATH\n",
    "3. Configure iPython notebook for Spark\n",
    "   * PYSPARK_DRIVER_PYTHON\n",
    "   * PYSPARK_DRIVER_PYTHON_OPTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oozie \n",
    "\n",
    "A tool to schedule workflows on all Hadoop ecosystem technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka\n",
    "\n",
    "A tool for stream processing for unbounded datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flink\n",
    "\n",
    "A tool for stream processing for unbounded datasets, similar to Kafka and SparkStreaming.\n",
    "\n",
    "Starts with a distinction:\n",
    "\n",
    "* Bounded datasets are processed in batches.\n",
    "* Unbounded datasets are processed in streams.\n",
    "\n",
    "*Unbounded datasets are those sets in which data is continously added until infinity.*\n",
    "\n",
    "#### Batch vs Stream Processing\n",
    "\n",
    "|Batch|Stream|\n",
    "|---|---|\n",
    "|Bounded, finite datasets|Unbounded, infinite datasets|\n",
    "|Slow pipeline from data ingestion to analysis|Processing immediate, as data is received|\n",
    "|Periodic updates as jobs complete|Continuous updates as jobs run constantly|\n",
    "|Order of data received unimportant|Order important, out of order arrival tracked|\n",
    "|Single global state at any point in time|No global state, only history of events received|\n",
    "\n",
    "#### Stream Processing\n",
    "\n",
    "1. Data is received as a stream (example: log messages, tweets, sensors)\n",
    "2. Process the data one entity at a time (example: filter error messages, find references to latest movies, track weather patterns)\n",
    "3. Must have a mechanism to store, display and act on filtered messages (example: trigger an alert, show trends, send warning)\n",
    "\n",
    "#### Stream-first Architecture\n",
    "\n",
    "* Used when data sources include streams\n",
    "* Has two components: **Message Transport** and **Stream Processing**\n",
    "\n",
    "#### Message Transport\n",
    "\n",
    "* Buffer for event data\n",
    "* High-performance and persistance\n",
    "* Decoupling multiple sources from processing\n",
    "* Kafka and MapR are Message Transport technologies\n",
    "\n",
    "#### Stream Processing\n",
    "\n",
    "* High throughput, low latency\n",
    "* Fault tolerance with low overhead\n",
    "* Manage out of order events (events that come at the wrong time)\n",
    "* Easy to use, maintenable\n",
    "* Ability to replay streams\n",
    "* SparkStreaming, Storm and Flink are Stream Processing technologies\n",
    "\n",
    "#### Microbatches\n",
    "\n",
    "* Is an approximation to stream processing\n",
    "* Grouping data in small batches. In the limit, as n_batches tends to infinity, Stream Processing emerges\n",
    "* Allows for exactly-once semantics, replay microbatches\n",
    "* Latency-throughput tradeoff based on batch sizes\n",
    "* SparkStreaming and Storm Trident use microbatches\n",
    "\n",
    "#### Microbatch Windows\n",
    "\n",
    "* Tumbling window\n",
    "  * Fixed window size\n",
    "  * Non-overlapping time\n",
    "  * Number of entities differ between windows\n",
    "* Sliding window\n",
    "  * Fixed window size\n",
    "  * Overlapping time - sliding interval\n",
    "  * Number of entities differ between windows\n",
    "* Session window\n",
    "  * Changing window size based on session data\n",
    "  * Non-overlapping time\n",
    "  * Number of entities differ between windows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
